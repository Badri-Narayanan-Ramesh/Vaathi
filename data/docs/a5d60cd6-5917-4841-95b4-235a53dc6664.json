{"doc_id": "a5d60cd6-5917-4841-95b4-235a53dc6664", "name": "test_pdf.pdf", "page_contexts": [{"page_id": 1, "raw_text": "EE641 Homework 2 \n \n \n \n \n \n \n \n \n \n \nVivin Thiyagarajan \n \nProblem 1 Analysis: \na) Why certain letters (like O, A) survive mode collapse while others (Q, X, Z) \ndisappear \n \n• Letters like A, C, and G show up more since their sharper edges make them easier to \nlearn. \n• X, Y, and Z still appear a bit, meaning the model did not completely collapse on them. \n• The feature-matching model handles this better overall, but both still miss some letters \nwith smoother shapes like O. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nFigure 1 \nb) Quantitative comparison of mode coverage with and without your chosen fix \n \n• Feature-matching GAN shows improved letter diversity compared to vanilla GAN \nbaseline (see Figure 2). \n• Vanilla GAN coverage drops to ~0.6-0.7 while feature-matching maintains ~0.7-0.8 \ncoverage. \n• Visual quality assessment shows feature-matching produces more recognizable letter \nshapes. \n", "ocr_text": "", "captions": ["a graph with the number of the letters and numbers"], "page_context": "RAW:\nEE641 Homework 2\n\n\nVivin Thiyagarajan\n\nProblem 1 Analysis:\na) Why certain letters (like O, A) survive mode collapse while others (Q, X, Z)\ndisappear\n\n• Letters like A, C, and G show up more since their sharper edges make them easier to\nlearn.\n• X, Y, and Z still appear a bit, meaning the model did not completely collapse on them.\n• The feature-matching model handles this better overall, but both still miss some letters\nwith smoother shapes like O.\n\n\nFigure 1\nb) Quantitative comparison of mode coverage with and without your chosen fix\n\n• Feature-matching GAN shows improved letter diversity compared to vanilla GAN\nbaseline (see Figure 2).\n• Vanilla GAN coverage drops to ~0.6-0.7 while feature-matching maintains ~0.7-0.8\ncoverage.\n• Visual quality assessment shows feature-matching produces more recognizable letter\nshapes.\n\n\nOCR:\n\n\nCAPTIONS:\na graph with the number of the letters and numbers", "tokens": 145}, {"page_id": 2, "raw_text": " \n• Training stability is significantly improved with less oscillation in loss curves and \ncoverage metrics. \nFigure 2 \nc)  Discussion of training dynamics: when does collapse begin? \n \n• Mode collapse becomes evident around epoch 20-30 when coverage begins sharp decline \n(see Figure 3). \n• Early epochs (0-10) show high apparent coverage due to random initialization diversity. \n• Mid-training (20-50) reveals the critical collapse period where generator converges to \nlimited modes. \n• Late training (50+) shows stabilization at reduced coverage levels with persistent mode \nlimitations. \n", "ocr_text": "", "captions": ["a graph with a line graph and a bar graph"], "page_context": "RAW:\n\n• Training stability is significantly improved with less oscillation in loss curves and\ncoverage metrics.\nFigure 2\nc) Discussion of training dynamics: when does collapse begin?\n\n• Mode collapse becomes evident around epoch 20-30 when coverage begins sharp decline\n(see Figure 3).\n• Early epochs (0-10) show high apparent coverage due to random initialization diversity.\n• Mid-training (20-50) reveals the critical collapse period where generator converges to\nlimited modes.\n• Late training (50+) shows stabilization at reduced coverage levels with persistent mode\nlimitations.\n\n\nOCR:\n\n\nCAPTIONS:\na graph with a line graph and a bar graph", "tokens": 96}, {"page_id": 3, "raw_text": " \n \n \n \n \n \n \n \n \n \n \n \nFigure 3 \nd) Evaluation of your chosen stabilization technique’s effectiveness \n• Feature-matching stabilization demonstrates measurable improvements in both \nmetrics and visual quality (see Figure 4). \n• The technique prevents discriminator overpowering by matching intermediate feature \ndistributions. \n• Generated samples show better letter recognition and reduced artifacts compared to \nvanilla baseline. \n• Training curves exhibit more stable convergence with reduced oscillation patterns. \n \n \n \n \n \n \n \n \n", "ocr_text": "", "captions": ["a graph graph shows the number of patients with a different type of cancer", "a graph graph with a blue rec and a blue rec"], "page_context": "RAW:\n\n\nFigure 3\nd) Evaluation of your chosen stabilization technique’s effectiveness\n• Feature-matching stabilization demonstrates measurable improvements in both\nmetrics and visual quality (see Figure 4).\n• The technique prevents discriminator overpowering by matching intermediate feature\ndistributions.\n• Generated samples show better letter recognition and reduced artifacts compared to\nvanilla baseline.\n• Training curves exhibit more stable convergence with reduced oscillation patterns.\n\n\nOCR:\n\n\nCAPTIONS:\na graph graph shows the number of patients with a different type of cancer\na graph graph with a blue rec and a blue rec", "tokens": 89}, {"page_id": 4, "raw_text": "Figure 4 \n \nProblem 2 Analysis: \na) Evidence of posterior collapse and how annealing prevented it \n \n• KL annealing and free bits prevented posterior collapse, as shown by nonzero KL and \ndiverse outputs (see Figure 1). \n• Latent variation experiments (complexity control, humanization) show clear changes \nin generated patterns. \n• If the model had collapsed, all patterns would look identical regardless of latent input. \n \nb) Interpretation of what each latent dimension learned to control \n \n• Sweeping a single latent dimension (complexity control) alters pattern density and \ninstrument activation (see Figure 2). \n• Each latent controls a distinct musical property, such as density, timing, or \ninstrument usage. \n• Humanization experiment shows subtle timing and instrument changes from \nlatent noise. \n", "ocr_text": "", "captions": ["a plot plot with a line graph and a plot plot with a line graph"], "page_context": "RAW:\nFigure 4\n\nProblem 2 Analysis:\na) Evidence of posterior collapse and how annealing prevented it\n\n• KL annealing and free bits prevented posterior collapse, as shown by nonzero KL and\ndiverse outputs (see Figure 1).\n• Latent variation experiments (complexity control, humanization) show clear changes\nin generated patterns.\n• If the model had collapsed, all patterns would look identical regardless of latent input.\n\nb) Interpretation of what each latent dimension learned to control\n\n• Sweeping a single latent dimension (complexity control) alters pattern density and\ninstrument activation (see Figure 2).\n• Each latent controls a distinct musical property, such as density, timing, or\ninstrument usage.\n• Humanization experiment shows subtle timing and instrument changes from\nlatent noise.\n\n\nOCR:\n\n\nCAPTIONS:\na plot plot with a line graph and a plot plot with a line graph", "tokens": 135}, {"page_id": 5, "raw_text": "c) Quality assessment: Do generated patterns sound musical? \n \n• Generated jazz patterns in Figure 3 show realistic timing and structure. \n• The rhythms capture jazz-specific traits like syncopation and complexity. \n• Each sample is rhythmically coherent with noticeable variation. \n \n \nd) Comparison of different annealing strategies \n \n• Figure 4 compares cyclical (top) and linear (bottom) annealing strategies in \ngenerated drum patterns. \n", "ocr_text": "", "captions": ["figure 5 - interpretation of the data", "figure 3 - sample of the first - genitaly of the first - genitaly of the first - genitaly of the first - gen"], "page_context": "RAW:\nc) Quality assessment: Do generated patterns sound musical?\n\n• Generated jazz patterns in Figure 3 show realistic timing and structure.\n• The rhythms capture jazz-specific traits like syncopation and complexity.\n• Each sample is rhythmically coherent with noticeable variation.\n\n\nd) Comparison of different annealing strategies\n\n• Figure 4 compares cyclical (top) and linear (bottom) annealing strategies in\ngenerated drum patterns.\n\n\nOCR:\n\n\nCAPTIONS:\nfigure 5 - interpretation of the data\nfigure 3 - sample of the first - genitaly of the first - genitaly of the first - genitaly of the first - gen", "tokens": 94}, {"page_id": 6, "raw_text": "• Cyclical annealing alternates between 0 and 1, giving periodic breaks from the KL \nconstraint and sustaining diversity. \n• Linear annealing increases monotonically from 0 to 1, often causing premature \ncollapse and repetitive patterns. \n• Overall, cyclical annealing maintains 2–3× higher pattern diversity and more \nmusically coherent outputs \n \n \n", "ocr_text": "", "captions": ["figure 1 cyclopin vs barotininin comparison comparison"], "page_context": "RAW:\n• Cyclical annealing alternates between 0 and 1, giving periodic breaks from the KL\nconstraint and sustaining diversity.\n• Linear annealing increases monotonically from 0 to 1, often causing premature\ncollapse and repetitive patterns.\n• Overall, cyclical annealing maintains 2–3× higher pattern diversity and more\nmusically coherent outputs\n\n\nOCR:\n\n\nCAPTIONS:\nfigure 1 cyclopin vs barotininin comparison comparison", "tokens": 58}, {"page_id": 7, "raw_text": "e) Success rate of style transfer while preserving rhythm \n \n• Figure 5 shows electronic samples with consistent rhythm and successful style \ntransfer. \n• Style is clearly adapted while core rhythmic structure remains intact. \n• High transfer success (>80%) observed across all tested genres. \n• Model maintains genre-specific style consistency while preserving rhythmic \ncontent. \n \n", "ocr_text": "", "captions": ["figure 5 electric samples"], "page_context": "RAW:\ne) Success rate of style transfer while preserving rhythm\n\n• Figure 5 shows electronic samples with consistent rhythm and successful style\ntransfer.\n• Style is clearly adapted while core rhythmic structure remains intact.\n• High transfer success (>80%) observed across all tested genres.\n• Model maintains genre-specific style consistency while preserving rhythmic\ncontent.\n\n\nOCR:\n\n\nCAPTIONS:\nfigure 5 electric samples", "tokens": 60}]}